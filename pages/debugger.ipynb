{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Desen\\Python311\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "f:\\Desen\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "f:\\Desen\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tinydb import TinyDB, Query\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.vectorstores import Chroma, pinecone\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"../docs\", glob=\"**/*.txt\")\n",
    "#loader = TextLoader(\"../docs/art_of_longevity.txt\")\n",
    "doc = loader.load()\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = doc[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop:\n",
    "            tokens.append(token.lemma_)\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "text = preprocess_text(text)\n",
    "doc[0].page_content = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, words_per_minute=115, duration=5):\n",
    "    words = text.split()\n",
    "    words_per_chunk = words_per_minute * duration\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), words_per_chunk):\n",
    "        chunk = words[i:i+words_per_chunk]\n",
    "        chunks.append(' '.join(chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text_into_chunks(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'( 60 ) Dr. Peter Attia â€ \" Science Art Longevity - YouTube https://www.youtube.com/watch?v=9l-_c8kcfR4 Transcript : ( 00:14 ) Tim Ferriss : Dr. Attia , Peter , nice . Dr. Peter Attia : good , man . Tim Ferriss : want start personal , sort topic , different aspect practice , lesson learn , et cetera house recently . Topo Chico bite eat thing . ( 00:35 ) comment large large obese , muscularly large . think begin question , necessarily order , add muscle mass . Dr. Peter Attia : fall 2021 , sort hit relative low weight low cyclist , probably 165 pound , pretty low maybe mid - low mid 170 . ( 01:13 ) DEXA scan , periodically , probably couple time year , year . DEXA scan , plot datum . interested body fat , bone mineral content metric . important Appendicular Lean Mass Index ALMI . ( 01:35 ) FFMI , Fat - Free Mass Index . probably worth explain little bit , ALMI DEXA scan impute lean tissue arm leg kilogram divide height meter square . spit number like kilogram meter square . ( 01:58 ) pretty accurate representation muscle arm leg DEXA scan identify thing , bone fat . thing arm bone fat tend muscle , pretty good identify muscle , Fat - Free Mass Index look entire body subtract fat . ( 02:19 ) good proxy lean mass , thing like organ resident . , look metric , shocked . shock muscle 10 year early , exactly 10 year soon . attribute thing , think chief fast previous year . ( 02:45 ) kind fast pretty extreme place 10 day water , routinely bust seven - day water - fast , certainly day month hesitation . Tim Ferriss : 10 day water , period time ? Dr. ( 03:08 ) Peter Attia : mean frequently ? Tim Ferriss : yeah , wonder cadence minimal â€ \" Dr. Peter Attia : probably quarterly . Tim Ferriss : . Dr. Peter Attia : yeah , probably quarter . certainly think benefit , think sort confront , wow , actually pretty big cost . ( 03:27 ) way fast exercise . lift weight single day , difficult , try stimulate muscle protein synthesis . end day , provide nutrient , include amino acid , basically tear muscle try rebuild muscle . ( 03:46 ) net effect loss . kind , \" \\' ve fix . \\' ve , want ALMI 90th percentile , 97th percentile age . \" datum pretty unambiguous people live long , live ALMI north 75th percentile . ( 04:10 ) clear bar , want considerably bar . basically change thing , \" nutrition exercise change little bit , \" talk change , basically \\' ve big change exercise nutrition correction . ( 04:28 ) Tim Ferriss : lean mass gain period time change ? Dr. Peter Attia : span , oh , God , look exact datum , believe 13 14 pound lean mass 12 month . Tim Ferriss : Mm - hmm . â€ \" Dr. ( 04:49 ) Peter Attia : , , herculean , right ? insane number , anabolic steroid , relatively doable high priority eat right protein , right kind protein , space consume right time , focus , train little hypertrophy . ( 05:13 ) Tim Ferriss : people listen , ask question simply interested lean mass , implication'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "#text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=400, chunk_overlap=0)\n",
    "#ldocs = text_splitter.split_documents(docs)\n",
    "text_spliter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=0)\n",
    "docs = text_spliter.create_documents(texts=[chunks[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "repo_id = \"stabilityai/stablelm-tuned-alpha-3b\"\n",
    "repo_id = \"databricks/dolly-v2-3b\"\n",
    "repo_id = \"Writer/camel-5b-hf\"\n",
    "repo_id = \"google/flan-t5-xl\"\n",
    "llm=HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\":0, \"max_length\":800}, huggingfacehub_api_token=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"( 60 ) Dr. Peter Attia â€ \" Science Art Longevity - YouTube https://www.youtube.com/watch?v=9l-_c8kcfR4 Transcript : ( 00:14 ) Tim Ferriss : Dr. Attia , Peter , nice . Dr. Peter Attia : good , man . Tim Ferriss : want start personal , sort topic , different aspect practice , lesson learn , et cetera house recently . Topo Chico bite eat thing . ( 00:35 ) comment large large obese , muscularly large\n",
      "\n",
      ". think begin question , necessarily order , add muscle mass . Dr. Peter Attia : fall 2021 , sort hit relative low weight low cyclist , probably 165 pound , pretty low maybe mid - low mid 170 . ( 01:13 ) DEXA scan , periodically , probably couple time year , year . DEXA scan , plot datum . interested body fat , bone mineral content metric . important Appendicular Lean Mass Index ALMI . ( 01:35 )\n",
      "\n",
      "FFMI , Fat - Free Mass Index . probably worth explain little bit , ALMI DEXA scan impute lean tissue arm leg kilogram divide height meter square . spit number like kilogram meter square . ( 01:58 ) pretty accurate representation muscle arm leg DEXA scan identify thing , bone fat . thing arm bone fat tend muscle , pretty good identify muscle , Fat - Free Mass Index look entire body subtract fat . (\n",
      "\n",
      "02:19 ) good proxy lean mass , thing like organ resident . , look metric , shocked . shock muscle 10 year early , exactly 10 year soon . attribute thing , think chief fast previous year . ( 02:45 ) kind fast pretty extreme place 10 day water , routinely bust seven - day water - fast , certainly day month hesitation . Tim Ferriss : 10 day water , period time ? Dr. ( 03:08 ) Peter Attia : mean\n",
      "\n",
      "frequently ? Tim Ferriss : yeah , wonder cadence minimal â€ \" Dr. Peter Attia : probably quarterly . Tim Ferriss : . Dr. Peter Attia : yeah , probably quarter . certainly think benefit , think sort confront , wow , actually pretty big cost . ( 03:27 ) way fast exercise . lift weight single day , difficult , try stimulate muscle protein synthesis . end day , provide nutrient , include amino acid ,\n",
      "\n",
      "basically tear muscle try rebuild muscle . ( 03:46 ) net effect loss . kind , \" ' ve fix . ' ve , want ALMI 90th percentile , 97th percentile age . \" datum pretty unambiguous people live long , live ALMI north 75th percentile . ( 04:10 ) clear bar , want considerably bar . basically change thing , \" nutrition exercise change little bit , \" talk change , basically ' ve big change exercise nutrition\n",
      "\n",
      "correction . ( 04:28 ) Tim Ferriss : lean mass gain period time change ? Dr. Peter Attia : span , oh , God , look exact datum , believe 13 14 pound lean mass 12 month . Tim Ferriss : Mm - hmm . â€ \" Dr. ( 04:49 ) Peter Attia : , , herculean , right ? insane number , anabolic steroid , relatively doable high priority eat right protein , right kind protein , space consume right time , focus , train\n",
      "\n",
      "little hypertrophy . ( 05:13 ) Tim Ferriss : people listen , ask question simply interested lean mass , implication\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain import  PromptTemplate, LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "chain = load_summarize_chain(llm, chain_type=\"stuff\", verbose=True)\n",
    "chain.run(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone \n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=\"60653ae3-5b1a-4963-897c-ffda44fa6f6e\",  # find at app.pinecone.io\n",
    "    environment=\"us-central1-gcp\"  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"outlive\"\n",
    "\n",
    "docsearch = Pinecone.from_documents(ldocs, embeddings, index_name=index_name)\n",
    "\n",
    "\n",
    "# if you already have an index, you can load it like this\n",
    "# docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "query = \"What is APO B\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outlive']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=\"60653ae3-5b1a-4963-897c-ffda44fa6f6e\",  # find at app.pinecone.io\n",
    "    environment=\"us-central1-gcp\"  # next to api key in console\n",
    ")\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='addition  to  statins;  the  combination  of  statins  plus  PCSK9  inhibitors  is  the  most\\n\\npowerful  pharmacological  tool  that  we  have  against  apoB.  Alas,  statins  do  not\\n\\nreduce  Lp(a),  but  PCSK9  inhibitors  do  in  most  patients,  typically  to  the  tune  of\\n\\nabout 30\\xa0percent.\\n\\nTriglycerides  also  contribute  to  the  apoB  particle  burden,  because  they  are\\n\\nlargely  transported  in  VLDLs.  Our  dietary  interventions  are  aimed  at  reducing\\n\\ntriglycerides, but in cases where nutritional changes are insufﬁcient, and in cases\\n\\nwhere  genetics  render  dietary  interventions  useless,  ﬁbrates  are  the  drug  of\\n\\nchoice.\\n\\nEthyl eicosapentaenoic acid (Vascepa), a drug derived from ﬁsh oil and\\n\\nconsisting  of  four  grams  of  pharmaceutical-grade  eicosapentaenoic  acid  (EPA),\\n\\nalso has FDA approval to reduce LDL in patients with elevated triglycerides.\\n\\nS\\x00\\x00\\x00 N\\x00\\x00\\x00\\x00\\n\\n1 There are also very-low-density lipoproteins, or VLDLs, which we mentioned in the previous\\n\\nchapter, as well as intermediate-density lipoproteins, or IDLs. These carry even more fat than the\\n\\nLDLs, much of it in the form of triglycerides, and they are also marked with apoB. Also: While HDL', metadata={'source': '..\\\\docs\\\\Outlive.pdf'}),\n",
       " Document(page_content='HDL  functionality,  and  until  we  have  a  better  grasp  on  how  HDL  actually\\n\\nworks, it will likely remain elusive as a target of therapy.\\n\\n—\\n\\nLipoproteins aren’t the only signiﬁcant risk factors for cardiovascular disease;\\n\\nas  noted  earlier,  smoking  and  high  blood  pressure  both  damage  the\\n\\nendothelium directly. Smoking cessation and blood pressure control are thus\\n\\nnon-negotiable ﬁrst steps in reducing cardiovascular risk.\\n\\nWe’ll  talk  about  nutrition  in  much  more  detail,  but  my  ﬁrst  step  in\\n\\ncontrolling my own cardiovascular risk was to begin to change my own diet,\\n\\nso as to lower my triglycerides (a contributor to apoB when they are high, as\\n\\nmine  were),  but  more  importantly  to  manage  my  insulin  levels.  I  needed  to\\n\\nget  my  metabolic  house  in  order.  I  should  note  that  my  own  solution  at  the\\n\\ntime, a ketogenic diet, might not work for everyone, nor is it a diet to which I\\n\\ncontinue to adhere. In my clinical experience, about a third to half of people\\n\\nwho consume high amounts of saturated fats (which sometimes goes hand in\\n\\nhand  with  a  ketogenic  diet)  will  experience  a  dramatic  increase  in  apoB\\n\\nparticles, which we obviously don’t want.[*8] Monounsaturated fats, found in\\n\\nhigh  quantities  in  extra  virgin  olive  oil,  macadamia  nuts,  and  avocados', metadata={'source': '..\\\\docs\\\\Outlive.pdf'}),\n",
       " Document(page_content='Allison, James,\\u2002160–161,\\u2002160n\\n\\nAlzheimer, Alois,\\u2002181,\\u2002182,\\u2002185\\n\\nAlzheimer’s disease\\n\\namyloid hypothesis,\\u2002182–185\\n\\namyloid hypothesis alternatives,\\u2002194–197\\n\\nAPOE gene variants (e2, e3, e4),\\u200267–69,\\u2002178–179,\\u2002180,\\u2002189,\\u2002196,\\n\\n197–199\\n\\nbackground,\\u200283,\\u2002102,\\u2002180–186\\n\\nconclusion,\\u2002204–205\\n\\nearly detection strategy,\\u2002178–179,\\u2002180\\n\\nMedicine 2.0 vs. Medicine 3.0 approaches,\\u2002188,\\u2002190\\n\\nprevention strategy,\\u2002186–193,\\u2002199–204,\\u2002225,\\u2002321,\\u2002326,\\u2002363\\n\\nrisk factors,\\u200217,\\u200271–72,\\u2002109,\\u2002188–189,\\u2002196\\n\\ntreatments targeting amyloid,\\u2002183–184\\n\\nAlzheimer’s Treatment, Alzheimer’s Prevention (Isaacson),\\u2002187\\n\\nAmbien,\\u2002364,\\u2002365,\\u2002366\\n\\nAMP-activated protein kinase (AMPK),\\u200282\\n\\namyloid-beta,\\u2002182–185,\\u2002195–196,\\u2002362–363\\n\\namyotrophic lateral sclerosis (ALS; Lou Gehrig’s disease),\\u200283,\\u2002179\\n\\nAndrews, Peter,\\u2002104–105\\n\\nAPOE gene variants (e2, e3, e4),\\u200267–69,\\u2002178–179,\\u2002180,\\u2002189,\\u2002196,\\u2002197–\\n\\n199', metadata={'source': '..\\\\docs\\\\Outlive.pdf'}),\n",
       " Document(page_content='apolipoprotein A (apoA),\\u2002117,\\u2002120\\n\\napolipoprotein B (apoB),\\u2002117,\\u2002117n,\\u2002120,\\u2002121,\\u2002133–134,\\u2002137\\n\\napolipoprotein B (apoB) test,\\u2002126–127,\\u2002130–131,\\u2002132\\n\\napolipoprotein E (APOE),\\u200267–68,\\u2002197–198\\n\\nAretaeus of Cappadocia,\\u2002102\\n\\narterial plaque,\\u2002122,\\u2002124–125\\n\\nAshkenazi Jewish centenarians,\\u200261\\n\\nashwagandha,\\u2002366\\n\\natherosclerotic cardiovascular disease (ASCVD). See\\u2002cardiovascular disease\\n\\natherosclerotic plaque,\\u2002122,\\u2002124–125\\n\\nAtlas of Atherosclerosis Progression and Regression (Stary),\\u2002119\\n\\nautomobile accident analogy,\\u2002212–213\\n\\nautophagy,\\u200282–83,\\u2002341\\n\\nB\\n\\nBacon, Francis,\\u200227\\n\\nballoon analogy,\\u2002101,\\u2002108\\n\\n“Barry Get-Up” ﬁtness assessment,\\u2002290\\n\\nBarzilai, Nir,\\u200261,\\u200264,\\u200287\\n\\nbathtub analogy,\\u200298\\n\\nB-cell lymphoma,\\u2002160\\n\\nbempedoic acid (Nexletol),\\u2002138\\n\\nBetts, James,\\u2002344\\n\\nBlessed, Garry,\\u2002181,\\u2002184,\\u2002194\\n\\nblood pressure. See\\u2002high blood pressure\\n\\nblood test for cancer (liquid biopsy),\\u2002145,\\u2002172–175,\\u2002172n\\n\\nBoorstin, Daniel J.,\\u2002177', metadata={'source': '..\\\\docs\\\\Outlive.pdf'})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: ../docs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vectordb = Chroma.from_documents(documents=ldocs, embedding=embeddings, collection_name=\"outlive_collection\", persist_directory=\"../docs\")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: ../docs\n"
     ]
    }
   ],
   "source": [
    "vectordb = None\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectordb = Chroma(embedding_function=embeddings, collection_name=\"outlive_collection\", persist_directory=\"../docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.similarity_search(\"Whats is APO B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: ../docs/\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "client = chromadb.Client(Settings(chroma_db_impl=\"duckdb+parquet\",\n",
    "                                    persist_directory=\"../docs/\"\n",
    "                                ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"outlive_collection\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.similarity_search('Explain in details APO b.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "repo_id = \"stabilityai/stablelm-tuned-alpha-3b\"\n",
    "repo_id = \"databricks/dolly-v2-3b\"\n",
    "repo_id = \"Writer/camel-5b-hf\"\n",
    "open_api_key = \"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\"\n",
    "llm=HuggingFaceHub(repo_id=\"databricks/dolly-v2-3b\", model_kwargs={\"temperature\":0, \"max_length\":400}, huggingfacehub_api_token=\"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\")\n",
    "llmopen = OpenAI(temperature=0, openai_api_key=\"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\")\n",
    "from langchain.chains import RetrievalQA\n",
    "chain = RetrievalQA.from_chain_type(llm=llmopen, \n",
    "                                    chain_type=\"map_reduce\", \n",
    "                                    retriever=docsearch.as_retriever(), \n",
    "                                    input_key=\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I don't know.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"book chapters?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llmopen = ChatOpenAI(temperature=\"0\", openai_api_key=open_api_key, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "tools = [\n",
    "\t\t\tTool(\n",
    "\t\t\t\tname=\"QA System\",\n",
    "\t\t\t\tfunc=chain.run,\n",
    "\t\t\t\tdescription=\"Useful for when you need to answer questions about the aspects asked. Input may be a partial or fully formed question.\",\n",
    "\t\t\t)\n",
    "\t\t]\n",
    "prefix = \"\"\"Have a conversation with a human, answering the following questions as best you can based on the context and memory available. \n",
    "                        You have access to a single tool:\"\"\"\n",
    "suffix = \"\"\"Begin!\"\n",
    "\n",
    "{chat_history}\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "\ttools,\n",
    "\tprefix=prefix,\n",
    "\tsuffix=suffix,\n",
    "\tinput_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "\t\tmemory_key=\"chat_history\"\n",
    "\t)\n",
    "\n",
    "# Chain\n",
    "# Zero shot agent\n",
    "# Agen Executor\n",
    "llm_chain = LLMChain(\n",
    "\tllm=llmopen,\n",
    "\tprompt=prompt,\n",
    ")\n",
    "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
    "agent_chain = AgentExecutor.from_agent_and_tools(\n",
    "\tagent=agent, tools=tools, verbose=True, memory=memory\n",
    ")\n",
    "\n",
    "query = \"Tell everything about APO b.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I'm not sure what APO b is, I need to use the QA System tool to find out more information.\n",
      "Action: QA System\n",
      "Action Input: \"What is APO b?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m APOE is a gene related to Alzheimer's disease risk.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThat doesn't seem to be the right answer. I should try rephrasing the question.\n",
      "Action: QA System\n",
      "Action Input: \"What is APOB?\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m ApoB is an apolipoprotein found on the surface of lipoproteins, such as very-low-density lipoproteins (VLDLs) and low-density lipoproteins (LDLs). It is used to measure the concentration of particles that can contribute to atherosclerosis.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mOkay, now I have a better understanding of what APO b is. I should provide this information to the person asking the question.\n",
      "Final Answer: APO b, or ApoB, is an apolipoprotein found on the surface of lipoproteins and is used to measure the concentration of particles that can contribute to atherosclerosis.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'APO b, or ApoB, is an apolipoprotein found on the surface of lipoproteins and is used to measure the concentration of particles that can contribute to atherosclerosis.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = agent_chain.run(query)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' APO B is a protein found on the surface of low-density lipoproteins (LDLs) and very-low-density lipoproteins (VLDLs). It is responsible for transporting fat molecules, such as cholesterol, throughout the body. High levels of APO B are associated with an increased risk of cardiovascular disease.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Please explain what is APO little b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Please explain what is APO b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, vectordb.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_history = []\n",
    "query = \"In chapter one tell me about fast death to slow death.\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"Can you give me more details? whats is 'b.'\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What NFL team won the Super Bowl in the year Justin Beiber was born?\n",
      "\n",
      "Answer: Let's think step by step.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Seattle Seahawks won the Super Bowl in 2010. Justin Beiber was born in 2010. The final answer: Seattle Seahawks.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
    "\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "\n",
    "llm_chain.predict(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Document' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m AnalyzeDocumentChain\n\u001b[0;32m      5\u001b[0m summarize_document_chain \u001b[39m=\u001b[39m AnalyzeDocumentChain(combine_docs_chain\u001b[39m=\u001b[39msummary_chain)\n\u001b[1;32m----> 6\u001b[0m summarize_document_chain\u001b[39m.\u001b[39;49mrun(docs)\n",
      "File \u001b[1;32mf:\\Desen\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[1;32mf:\\Desen\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mf:\\Desen\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    109\u001b[0m     inputs,\n\u001b[0;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mf:\\Desen\\Python311\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:95\u001b[0m, in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     92\u001b[0m text_splitter: TextSplitter \u001b[39m=\u001b[39m Field(default_factory\u001b[39m=\u001b[39mRecursiveCharacterTextSplitter)\n\u001b[0;32m     93\u001b[0m combine_docs_chain: BaseCombineDocumentsChain\n\u001b[1;32m---> 95\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minput_keys\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Expect input key.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \n\u001b[0;32m     99\u001b[0m \u001b[39m    :meta private:\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key]\n",
      "File \u001b[1;32mf:\\Desen\\Python311\\Lib\\site-packages\\langchain\\text_splitter.py:54\u001b[0m, in \u001b[0;36mcreate_documents\u001b[1;34m(self, texts, metadatas)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create documents from a list of texts.\"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m _metadatas \u001b[39m=\u001b[39m metadatas \u001b[39mor\u001b[39;00m [{}] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(texts)\n\u001b[1;32m---> 54\u001b[0m documents \u001b[39m=\u001b[39m []\n\u001b[0;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m i, text \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(texts):\n\u001b[0;32m     56\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_text(text):\n",
      "File \u001b[1;32mf:\\Desen\\Python311\\Lib\\site-packages\\langchain\\text_splitter.py:266\u001b[0m, in \u001b[0;36msplit_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create a new TextSplitter.\"\"\"\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 266\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_separators \u001b[39m=\u001b[39m separators \u001b[39mor\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'Document' has no len()"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "summary_chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "from langchain.chains import AnalyzeDocumentChain\n",
    "summarize_document_chain = AnalyzeDocumentChain(combine_docs_chain=summary_chain)\n",
    "summarize_document_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Desen\\Python311\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "f:\\Desen\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "f:\\Desen\\Python311\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bbd16dcbe24d61bb6c1e42ce1cfbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Desen\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\giuice\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e521e84588443eb17e12da44fd1912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Something else you want to summarize with BERT'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "\n",
    "body = 'Text body that you want to summarize with BERT'\n",
    "body2 = 'Something else you want to summarize with BERT'\n",
    "model = Summarizer()\n",
    "model(body)\n",
    "model(body2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text body that you want to summarize with BERT\n"
     ]
    }
   ],
   "source": [
    "from summarizer.sbert import SBertSummarizer\n",
    "\n",
    "body = 'Text body that you want to summarize with BERT'\n",
    "model = SBertSummarizer('paraphrase-MiniLM-L6-v2')\n",
    "result = model(body, num_sentences=3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting neuralcoref\n",
      "  Downloading neuralcoref-4.0.tar.gz (368 kB)\n",
      "                                              0.0/368.7 kB ? eta -:--:--\n",
      "     ---------------------                  204.8/368.7 kB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------                  204.8/368.7 kB 4.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 368.7/368.7 kB 2.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.15.0 in f:\\desen\\python311\\lib\\site-packages (from neuralcoref) (1.23.5)\n",
      "Collecting boto3 (from neuralcoref)\n",
      "  Downloading boto3-1.26.129-py3-none-any.whl (135 kB)\n",
      "                                              0.0/135.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 135.6/135.6 kB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in f:\\desen\\python311\\lib\\site-packages (from neuralcoref) (2.28.2)\n",
      "Requirement already satisfied: spacy>=2.1.0 in f:\\desen\\python311\\lib\\site-packages (from neuralcoref) (3.5.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\desen\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\desen\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\desen\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\desen\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2022.12.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (4.65.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.1.2)\n",
      "Requirement already satisfied: setuptools in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (67.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\giuice\\appdata\\roaming\\python\\python311\\site-packages (from spacy>=2.1.0->neuralcoref) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in f:\\desen\\python311\\lib\\site-packages (from spacy>=2.1.0->neuralcoref) (3.3.0)\n",
      "Collecting botocore<1.30.0,>=1.29.129 (from boto3->neuralcoref)\n",
      "  Downloading botocore-1.29.129-py3-none-any.whl (10.7 MB)\n",
      "                                              0.0/10.7 MB ? eta -:--:--\n",
      "     -                                        0.5/10.7 MB 10.0 MB/s eta 0:00:02\n",
      "     ---                                      0.9/10.7 MB 9.3 MB/s eta 0:00:02\n",
      "     ----                                     1.3/10.7 MB 9.2 MB/s eta 0:00:02\n",
      "     ------                                   1.7/10.7 MB 8.8 MB/s eta 0:00:02\n",
      "     -------                                  2.1/10.7 MB 9.4 MB/s eta 0:00:01\n",
      "     ---------                                2.5/10.7 MB 9.3 MB/s eta 0:00:01\n",
      "     ----------                               2.9/10.7 MB 9.3 MB/s eta 0:00:01\n",
      "     ------------                             3.4/10.7 MB 9.5 MB/s eta 0:00:01\n",
      "     --------------                           3.9/10.7 MB 9.5 MB/s eta 0:00:01\n",
      "     ----------------                         4.3/10.7 MB 9.5 MB/s eta 0:00:01\n",
      "     -----------------                        4.7/10.7 MB 9.4 MB/s eta 0:00:01\n",
      "     -------------------                      5.2/10.7 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------                    5.7/10.7 MB 9.6 MB/s eta 0:00:01\n",
      "     ----------------------                   6.2/10.7 MB 9.6 MB/s eta 0:00:01\n",
      "     ------------------------                 6.7/10.7 MB 9.7 MB/s eta 0:00:01\n",
      "     --------------------------               7.1/10.7 MB 9.6 MB/s eta 0:00:01\n",
      "     ----------------------------             7.7/10.7 MB 9.8 MB/s eta 0:00:01\n",
      "     -----------------------------            8.0/10.7 MB 9.7 MB/s eta 0:00:01\n",
      "     -------------------------------          8.4/10.7 MB 9.5 MB/s eta 0:00:01\n",
      "     -------------------------------          8.4/10.7 MB 9.5 MB/s eta 0:00:01\n",
      "     --------------------------------         8.6/10.7 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------------------------        8.9/10.7 MB 8.8 MB/s eta 0:00:01\n",
      "     ----------------------------------       9.3/10.7 MB 8.7 MB/s eta 0:00:01\n",
      "     -----------------------------------      9.6/10.7 MB 8.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    9.9/10.7 MB 8.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   10.3/10.7 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.7/10.7 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  10.7/10.7 MB 8.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 10.7/10.7 MB 8.1 MB/s eta 0:00:00\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->neuralcoref)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->neuralcoref)\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "                                              0.0/79.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 79.8/79.8 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\giuice\\appdata\\roaming\\python\\python311\\site-packages (from botocore<1.30.0,>=1.29.129->boto3->neuralcoref) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in f:\\desen\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=2.1.0->neuralcoref) (4.5.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in f:\\desen\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.1.0->neuralcoref) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in f:\\desen\\python311\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.1.0->neuralcoref) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\giuice\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.1.0->neuralcoref) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in f:\\desen\\python311\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy>=2.1.0->neuralcoref) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\desen\\python311\\lib\\site-packages (from jinja2->spacy>=2.1.0->neuralcoref) (2.1.2)\n",
      "Requirement already satisfied: six>=1.5 in f:\\desen\\python311\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.129->boto3->neuralcoref) (1.12.0)\n",
      "Building wheels for collected packages: neuralcoref\n",
      "  Building wheel for neuralcoref (setup.py): started\n",
      "  Building wheel for neuralcoref (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for neuralcoref\n",
      "Failed to build neuralcoref\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py bdist_wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [33 lines of output]\n",
      "      f:\\Desen\\Python311\\Lib\\site-packages\\setuptools\\__init__.py:85: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated. Requirements should be satisfied by a PEP 517 installer. If you are using pip, you can try `pip install --use-pep517`.\n",
      "        dist.fetch_build_eggs(dist.setup_requires)\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\neuralcoref\n",
      "      copying neuralcoref\\file_utils.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\n",
      "      copying neuralcoref\\__init__.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\n",
      "      creating build\\lib.win-amd64-cpython-311\\neuralcoref\\tests\n",
      "      copying neuralcoref\\tests\\test_neuralcoref.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\tests\n",
      "      copying neuralcoref\\tests\\__init__.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\tests\n",
      "      creating build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\algorithm.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\compat.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\conllparser.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\dataset.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\document.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\evaluator.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\learn.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\model.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\utils.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      copying neuralcoref\\train\\__init__.py -> build\\lib.win-amd64-cpython-311\\neuralcoref\\train\n",
      "      running build_ext\n",
      "      building 'neuralcoref.neuralcoref' extension\n",
      "      creating build\\temp.win-amd64-cpython-311\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\n",
      "      creating build\\temp.win-amd64-cpython-311\\Release\\neuralcoref\n",
      "      \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -If:\\Desen\\Python311\\include -IC:\\Users\\giuice\\AppData\\Local\\Temp\\pip-install-3mugvrkv\\neuralcoref_e4214d6dfbb7493a9d16291a81c4ea97\\include -If:\\Desen\\Python311\\include -If:\\Desen\\Python311\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\BuildTools\\VC\\Tools\\MSVC\\14.29.30133\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\ucrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\shared\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\um\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\winrt\" \"-IC:\\Program Files (x86)\\Windows Kits\\10\\include\\10.0.19041.0\\cppwinrt\" /EHsc /Tpneuralcoref/neuralcoref.cpp /Fobuild\\temp.win-amd64-cpython-311\\Release\\neuralcoref/neuralcoref.obj /Ox /EHsc\n",
      "      neuralcoref.cpp\n",
      "      neuralcoref/neuralcoref.cpp(196): fatal error C1083: Cannot open include file: 'longintrepr.h': No such file or directory\n",
      "      error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2019\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.29.30133\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for neuralcoref\n",
      "ERROR: Could not build wheels for neuralcoref, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "%pip install neuralcoref\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
